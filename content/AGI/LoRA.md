+++
date = '2025-06-01T17:07:50+08:00'
draft = false
title = 'LoRA'
summary= "对论文《LoRA: Low-Rank Adaptation of Large Language Models》的研究动机、方法设计、实验验证及深入分析等方面进行了详细剖析"
+++

---

## 1. 研究背景与动机

当下，自然语言处理领域普遍采取“大规模预训练＋下游微调”的范式。然而，随着模型规模不断增大，**全量微调(fine-tuning)** 面临以下两大挑战：

1. **存储与部署成本高昂**：例如在 GPT-3 175B（约1750亿参数）上进行全量微调时，每个任务都需要保存一套完整的 175B 参数，若同时支持多个任务，则需要占用数 TB 级别的存储空间，且加载模型实例时开销巨大。
2. **训练资源门槛升高**：对于 Adam 这样的自适应优化器，需要为所有参数维护梯度和优化器状态，GPT-3 175B 全量微调时，显存占用高达约 1.2TB，这对大多数团队和硬件环境几乎无法承受。

为缓解上述问题，既往研究尝试**冻结部分权重**或通过引入外部适配器（adapter）/前缀（prompt/prefix）等模块来降低可训练参数规模。但这些方法往往会带来以下弊端：

* **Adapter 方法增加推理时延**：在 Transformer 层间插入一个小规模的瓶颈层（Adapter），虽参数量少，但必须在正向推理时进行额外计算，在线推理时（尤其 Batch Size=1 情况下）延迟不容忽视。
* **Prompt/Prefix 方法难以优化且占用输入长度**：这类方法通过在输入序列中插入特殊标记，使模型在前向时多处理一段“可训练”激活；但随着特殊标记数目增多，性能提升并不稳定，而且会减少可用于下游任务的实际 Token 数。

鉴于此，论文提出一种新的 **低秩适配(LoRA, Low-Rank Adaptation)** 策略：在保持预训练权重不变的前提下，通过向每层注入低秩分解矩阵来进行微调，从而在不增加推理时延、不缩减序列长度的情况下，大幅降低可训练参数量和显存占用。

---

## 2. 问题定义

假设已得到一个预训练的自回归语言模型 $P_\Phi(y\mid x)$，其参数为 $\Phi_0$。在下游任务中，往往通过最大化条件语言建模目标进行全量微调：

$$
\max_{\Phi}\sum_{(x,y)\in \mathcal{Z}}\sum_{t=1}^{|y|} \log P_{\Phi}(y_t)(y_t \mid x, y_{< t}) \quad,\quad \Phi = \Phi_0 + \Delta \Phi
$$

此时需为每个下游任务保存一份 $\Delta \Phi$，且 $|\Delta \Phi| = |\Phi_0|$。当 $\Phi_0$ 较大（如 GPT-3 175B）时，无论是**存储**还是**训练显存**都几近或超出可承受范围。

论文改写该问题为：寻找一个仅依赖于少量参数 $\Theta$ 的更新映射 $\Delta \Phi(\Theta)$ 使得

$$
\max_{\Theta}\sum_{(x,y)\in \mathcal{Z}}\sum_{t=1}^{|y|} \log P_{\Phi_0 + \Delta \Phi(\Theta)}(y_t \mid x, y_{< t}) \quad,\quad |\Theta| \ll |\Phi_0|.
$$

最终目标是在**显存占用**与**可训练参数量**大幅下降的同时，仍保持或超越全量微调的模型性能。

---

## 3. LoRA 方法

### 3.1 核心思想：低秩分解更新

LoRA 的核心假设是：在大规模预训练模型的下游微调过程中，**权重变化 $\Delta W$ 具有“低秩”特性**，即若原始权重矩阵 $W_0\in \mathbb{R}^{d\times k}$ 进行更新因子 $\Delta W$，则可以近似为低秩分解

$$
W_0 + \Delta W = W_0 + B A,
\quad
B \in \mathbb{R}^{d \times r},\;
A \in \mathbb{R}^{r \times k},\;
r \ll \min(d, k).
$$

其中，$W_0$ 冻结不更新，仅对矩阵 $A, B$ 进行梯度更新。这样就将原来需要更新的 $d\times k$ 参数量，压缩为 $d\times r + r\times k\approx 2dr$ 个参数。

在前向计算时，对于某一层输入向量 $x\in \mathbb{R}^k$，原本输出为 $h = W_0 x$。采用 LoRA 后，输出变为

$$
h = W_0 x + \Delta W\,x = W_0 x + B (A x).
\tag{3}
$$

因此在实现上，只需在每次前向时多做一个 $A x$ 然后左乘 $B$，并与原始输出相加即可。论文实施时将 $B$ 初始化为零矩阵，$A$ 初始化为服从高斯分布的小随机值，保证微调初期 $\Delta W = BA$ 为零；训练过程中，会对 $A,B$ 应用缩放系数 $\alpha/r$（对相同的学习率而言相当于缩放初始值），以方便在不同秩 $r$ 下无需大幅调参而保持收敛稳定。

#### 3.1.1 与全量微调的关系

* 当 $r = \min(d,k)$ 时，等价于对原始权重矩阵 $W_0$ 进行全秩微调，相当于完整的 $\Delta W$；
* 当采用更小的 $r$ 时，只训练低秩子空间中的更新，从而实现**与全量微调相似的表达力**，但参数量和计算开销大幅减少。

---

### 3.2 在 Transformer 中的应用

论文将 LoRA 主要应用于 Transformer 结构中**自注意力模块（self-attention）**的投影矩阵：查询矩阵 $W_q$、键矩阵 $W_k$、值矩阵 $W_v$、输出矩阵 $W_o$ 以及前馈网络（MLP）部分的全连接权重矩阵。具体做法如下：

1. **仅对注意力投影矩阵应用 LoRA**：论文在大多数实验中只在 $W_q$ 和 $W_v$ 上引入低秩更新矩阵 $\Delta W_q = B_q A_q$，$\Delta W_v = B_v A_v$，并冻结其它权重（包括 MLP 层和层归一化层等）。实验证明，这样已经能够获得与全量微调相当或更优的性能。
2. **保持正向推理时延不变**：在部署推理时，只需将训练好的 $\Delta W$ 与 $W_0$ 合并为 $W_0 + BA$，正向计算流程与微调后模型完全一致，没有新增任何顺序化计算，因此**不会增加推理时延**。
3. **灵活切换任务**：将 $\Delta W$ 合并后对应于某个任务的权重写入硬盘，多个任务只需维护一份基础预训练权重与若干份低秩适配参数（大小通常为几 MB），切换任务时仅载入对应的 $\Delta W$，显存和存储占用极小。

| 应用位置  | 维度          | 低秩参数量 $\displaystyle 2\times d\times r$ | 说明         |
| ----- | ----------- | --------------------------------------- | ---------- |
| $W_q$ | $d\times d$ | $2dr$                                   | 查询投影，只改前两项 |
| $W_v$ | $d\times d$ | $2dr$                                   | 值投影        |
| …     | …           | …                                       | …          |

实践中，仅在 $W_q, W_v$ 上应用 $r=4$ 时（共计适配参数约数 MB 级别），即可满足 GPT-3 175B各种下游任务的性能需求。

---

## 4. LoRA 的实用优势与局限

### 4.1 优势

1. **显存占用降低**

   * 由于大部分权重冻结，不需为其维护梯度或优化器状态，仅对低秩矩阵 $A,B$ 更新。以 GPT-3 175B 为例，LoRA 微调时显存从约 1.2 TB 降至约 350 GB，节省约 3 倍显存。

2. **可训练参数量大幅减少**

   * 对 GPT-3 175B，仅在 $W_q,W_v$ 部分以 $r=4$ 方式注入，则低秩参数量约为 4.7 M，相比全量微调（175 B）减少约 10 000 ×。

3. **推理时延不增加**

   * 预先合并 $\Delta W = B A$ 到权重后，推理流程与全量微调相同，不会引入顺序化计算。与 Adapter 方法相比，Adapter 在每层插入瓶颈层会造成推理时间因顺序运算大幅增长，而 LoRA 不存在此缺陷。

4. **任务切换便捷**

   * 共享同一份预训练权重，在不同任务间只需加载不同的几 MB 级别的低秩适配参数，存储和部署成本极低。

5. **训练吞吐量提升**

   * 由于冻结大部分权重，反向传播计算量下降，在相同硬件下可获得约 25% 的训练速度提升。

### 4.2 局限

1. **批量多任务训练不便**

   * 若想在一个 Batch 中同时对不同任务样本使用不同的 $\Delta W$（即 A,B 不同），需在正向时动态切换权重，或者不合并权重，此时推理/训练流程复杂。论文指出若对延迟敏感，可以选择动态加载 LoRA 模块，否则需将不同 $\Delta W$ 合并到原权重，从而无法混合多任务一起正向。

2. **低秩假设或不适用所有任务**

   * 当下游任务与预训练任务域差异极大（例如全新语言、跨模态任务）时，所需更新可能并不低秩，此时 LoRA 的表达能力可能受限。论文在 GPT-3 上实验发现对于大部分 NLP 任务，仅需 $r=1$ 即可；但也提示并非所有任务都适用，需要适度调节秩 $r$。

---

## 5. 实验验证

论文在四种主流 Transformer 结构上进行了大规模实验：RoBERTa、DeBERTa、GPT-2 以及 GPT-3 175B。下文中，对照各项结果进行说明。

### 5.1 基线方法

| 基线名称                             | 说明                                                                                 |
| -------------------------------- | ---------------------------------------------------------------------------------- |
| **Full Fine-Tuning（FT）**         | 对所有权重和偏置进行微调。                                                                      |
| **BitFit**                       | 仅训练偏置向量（bias），其它权重冻结。                                                              |
| **Adapter (Houlsby 等)**          | 在自注意力和 MLP 后插入瓶颈 Adapter 层。AdapterL/AdapterH 两种变体，分别对应单适配器或双适配器结构，参数量约为原模型的 1% 左右。 |
| **Prefix-Tuning/Embedding (PE)** | 在输入/层激活前加入 lp + li 个可训练 Token 激活，等同于动态插入额外可训练参数，无法并行优化且会占用下游任务的序列长度。               |
| **Prefix-Layer (PL)**            | 类似于 PrefixEmbed，但对各层插入可训练的激活向量，同样占用计算和序列长度。                                        |

### 5.2 RoBERTa Base/Large 上的 GLUE 基准

论文在 GLUE 数据集（包含 MNLI、SST-2、MRPC、CoLA、QNLI、QQP、RTE、STS-B）上对 RoBERTa base (125M 参数) 和 RoBERTa large (355M 参数) 进行了多种方法对比。部分结果汇总如下（指标为各任务对应分数，整体为平均得分；越高越好）：

| 模型 (RoBERTa Base)     | 可训练参数 | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP  | RTE  | STS-B | Avg. |   |
| --------------------- | ----- | ---- | ----- | ---- | ---- | ---- | ---- | ---- | ----- | ---- | - |
| FT (125M)             | 125 M | 87.6 | 94.8  | 90.2 | 63.6 | 92.8 | 91.9 | 78.7 | 91.2  | 86.4 |   |
| BitFit (0.1 M)        | 0.1 M | 84.7 | 93.7  | 92.7 | 62.0 | 91.8 | 84.0 | 81.5 | 90.8  | 85.2 |   |
| AdapterD (0.3 M)      | 0.3 M | 87.1 | 94.2  | 88.5 | 60.8 | 93.1 | 90.2 | 71.5 | 89.7  | 84.4 |   |
| AdapterD (0.9 M)      | 0.9 M | 87.3 | 94.7  | 88.4 | 62.6 | 93.0 | 90.6 | 75.9 | 90.3  | 85.4 |   |
| **LoRA (r=4, 0.3 M)** | 0.3 M | 87.5 | 95.1  | 89.7 | 63.4 | 93.3 | 90.8 | 86.6 | 91.5  | 87.2 |   |

| 模型 (RoBERTa Large)    | 可训练参数 | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP  | RTE  | STS-B | Avg. |   |
| --------------------- | ----- | ---- | ----- | ---- | ---- | ---- | ---- | ---- | ----- | ---- | - |
| FT (355 M)            | 355 M | 90.2 | 96.4  | 90.9 | 68.0 | 94.7 | 92.2 | 86.6 | 92.4  | 88.9 |   |
| **LoRA (r=8, 0.8 M)** | 0.8 M | 90.6 | 96.2  | 90.9 | 68.2 | 94.9 | 91.6 | 87.4 | 92.6  | 89.0 |   |
| AdptP (3.0 M)         | 3.0 M | 90.2 | 96.1  | 90.2 | 68.3 | 94.8 | 91.9 | 83.8 | 92.1  | 88.4 |   |
| AdptP (0.8 M)         | 0.8 M | 90.5 | 96.6  | 89.7 | 67.8 | 94.8 | 91.7 | 80.1 | 91.9  | 87.9 |   |
| AdptH (6.0 M)         | 6.0 M | 89.9 | 96.2  | 88.7 | 66.5 | 94.7 | 92.1 | 83.4 | 91.0  | 87.8 |   |
| AdptH (0.8 M)         | 0.8 M | 90.3 | 96.3  | 87.7 | 66.3 | 94.7 | 91.5 | 72.9 | 91.5  | 86.4 |   |
| **LoRA (0.8 M)**      | 0.8 M | 90.6 | 96.2  | 90.2 | 68.2 | 94.8 | 91.6 | 85.2 | 92.3  | 88.6 |   |

从上表可以看出，**LoRA 在仅用 0.3 M—0.8 M 可训练参数的情况下，几乎匹配甚至略超全量微调**，且优于其他几种参数高效微调方法（如 Adapter、BitFit 等）。

---

### 5.3 DeBERTa XXL 上的 GLUE 基准

DeBERTa XXL（约 1.5B 参数）也是较大规模可用模型。论文在 GLUE 任务上对比了下列设置：

| 模型 (DeBERTa XXL)      | 可训练参数  | MNLI | SST-2 | MRPC | CoLA | QNLI | QQP  | RTE  | STS-B | Avg. |   |
| --------------------- | ------ | ---- | ----- | ---- | ---- | ---- | ---- | ---- | ----- | ---- | - |
| FT (1.5 B)            | 1500 M | 91.8 | 97.2  | 92.0 | 72.0 | 96.0 | 92.7 | 93.9 | 92.9  | 91.1 |   |
| **LoRA (r=8, 4.7 M)** | 4.7 M  | 91.9 | 96.9  | 92.6 | 72.4 | 96.0 | 92.9 | 94.9 | 93.0  | 91.3 |   |

结果显示，LoRA 仅用 4.7 M 可训练参数，就能略微超过全量微调的 DeBERTa XXL，且比 Adapter 等方法表现更稳定。

---

### 5.4 GPT-2 Medium/Large 上的 NLG 任务

针对生成任务，论文在 E2E NLG Challenge、WebNLG、DART 等数据集上对比了以下方法。以 E2E NLG Challenge（指标包括 BLEU、NIST、METEOR、ROUGE-L、CIDEr；越高越好）为例：

| 模型 & 方法                          | 可训练参数   | BLEU     | NIST     | METEOR     | ROUGE-L    | CIDEr      |   |
| -------------------------------- | ------- | -------- | -------- | ---------- | ---------- | ---------- | - |
| GPT-2 Medium (FT, 354.9 M)       | 354.9 M | 68.2     | 8.62     | 0.4565     | 0.710      | 2.47       |   |
| GPT-2 Medium (AdapterL, 0.37 M)  | 0.37 M  | 66.3     | 8.41     | 0.450      | 0.698      | 2.40       |   |
| GPT-2 Medium (AdapterL, 11.09 M) | 11.09 M | 68.9     | 8.71     | 0.461      | 0.713      | 2.47       |   |
| GPT-2 Medium (AdapterH, 11.09 M) | 11.09 M | 67.3     | 8.50     | 0.460      | 0.707      | 2.44       |   |
| GPT-2 Medium (FTTop2, 25.19 M)   | 25.19 M | 68.1     | 8.59     | 0.460      | 0.708      | 2.41       |   |
| GPT-2 Medium (PreLayer, 0.35 M)  | 0.35 M  | 69.7     | 8.81     | 0.461      | 0.714      | 2.49       |   |
| **GPT-2 Medium (LoRA, 0.35 M)**  | 0.35 M  | **70.4** | **8.85** | **0.4689** | **0.7186** | **2.5349** |   |

| 模型 & 方法                         | 可训练参数   | BLEU     | NIST     | METEOR     | ROUGE-L   | CIDEr    |   |
| ------------------------------- | ------- | -------- | -------- | ---------- | --------- | -------- | - |
| GPT-2 Large (FT, 774.0 M)       | 774.0 M | 68.5     | 8.78     | 0.460      | 0.699     | 2.45     |   |
| GPT-2 Large (AdapterL, 0.88 M)  | 0.88 M  | 69.1     | 8.68     | 0.463      | 0.714     | 2.49     |   |
| GPT-2 Large (AdapterL, 23.00 M) | 23.00 M | 68.9     | 8.70     | 0.461      | 0.713     | 2.45     |   |
| GPT-2 Large (PreLayer, 0.77 M)  | 0.77 M  | 70.3     | 8.85     | 0.462      | 0.717     | 2.47     |   |
| **GPT-2 Large (LoRA, 0.77 M)**  | 0.77 M  | **70.4** | **8.89** | **0.4689** | **0.720** | **2.47** |   |

由此可见，**LoRA 在仅 0.3 M—0.8 M 可训练参数的情况下，取得了与全量微调相当甚至略优的生成质量**。

---

### 5.5 GPT-3 175B 上的规模化实验

最后，论文将 LoRA 扩展至 GPT-3 175B，重点验证其在大模型下的可行性及性能。实验包括 WikiSQL（NL→SQL）、MultiNLI (MNLI-matched) 和 SAMSum（三元组对话摘要）三种任务，结果如下：

| 方法                       | 可训练参数       | WikiSQL (Acc. %) | MultiNLI-m (Acc. %) | SAMSum (Rouge-1/2/L) |   |
| ------------------------ | ----------- | ---------------- | ------------------- | -------------------- | - |
| GPT-3 (FT)               | 175 255.8 M | 73.8             | 89.5                | 52.0/28.0/44.5       |   |
| GPT-3 (BitFit, 14.2 M)   | 14.2 M      | 71.3             | 91.0                | 51.3/27.4/43.5       |   |
| GPT-3 (PreEmbed, 3.2 M)  | 3.2 M       | 63.1             | 88.6                | 48.3/24.2/40.5       |   |
| GPT-3 (PreLayer, 20.2 M) | 20.2 M      | 70.1             | 89.5                | 50.8/27.3/43.5       |   |
| GPT-3 (AdapterH, 7.1 M)  | 7.1 M       | 71.9             | 89.8                | 53.0/28.9/44.8       |   |
| GPT-3 (AdapterH, 40.1 M) | 40.1 M      | 73.2             | 91.5                | 53.2/29.0/45.1       |   |
| **GPT-3 (LoRA, 4.7 M)**  | 4.7 M       | 73.4             | 91.7                | 53.8/29.8/45.9       |   |
| **GPT-3 (LoRA, 37.7 M)** | 37.7 M      | 74.0             | 91.6                | 53.4/29.2/45.1       |   |

可以看出，**LoRA 在极少参数（4.7 M）情况下，已经逼近或超越了全量微调（175 B）的性能**，并且随着可训练参数量增加（37.7 M），任务性能略有提升但基本趋于稳定。实验还给出了不同方法在可训练参数数目上的性能曲线（图 2），LoRA 的表现明显更为平滑和可扩展。

---

## 6. 对 LoRA 更新矩阵的深入分析

针对为何低秩更新能取得上述效果，论文进行了多项实证研究，探讨更新矩阵 $\Delta W$ 的秩及与原权重 $W$ 之间的关系。以下是关键结论：

### 6.1 选取哪些权重进行 LoRA 更优

在 GPT-3 175B 上设定约 18 M 参数预算，实验对比了在不同注意力矩阵上分别应用 LoRA 的效果（秩 $r$ 根据权重数目反算，适配 $W_q, W_v$ 时取 $r=4$，仅适配单矩阵时取 $r=8$）。结果（表 5）显示：

| 适配位置                    | 可训练参数约 18 M | WikiSQL (Acc. %) | MNLI(m) (Acc. %) |   |
| ----------------------- | ----------- | ---------------- | ---------------- | - |
| 仅 $W_q$ (r=8)           | 18 M        | 68.8—70.4        | 90.7—90.9        |   |
| 仅 $W_k$ (r=8)           | 18 M        | 70.0             | 90.8             |   |
| 仅 $W_v$ (r=8)           | 18 M        | 73.0             | 91.0             |   |
| 仅 $W_o$ (r=8)           | 18 M        | 73.2             | 91.3             |   |
| $W_q,W_k$ (各 r=4)       | 18 M        | 71.4             | 91.3             |   |
| **$W_q,W_v$ (各 r=4)**   | 18 M        | **73.7**         | **91.3**         |   |
| $W_q,W_k,W_v,W_o$ (r=2) | 18 M        | 73.7             | 91.7             |   |

可见，在预算固定时，**同时适配 $W_q$ 与 $W_v$（各 $r=4$）能够获得最佳整体表现**。单独适配某一矩阵往往次优，比如仅适配 $W_q$ 需要更高 $r$ 才能接近效果。

---

### 6.2 更新矩阵是否真低秩？最佳秩 $r$ 的选择

实验进一步在 GPT-3 上考察了不同秩 $r$ 对下游任务性能的影响。以同时适配 $\{W_q,W_v\}$ 为例，在 WikiSQL 和 MNLI 上，当 $r$ 取 1、2、4、8、64 时，性能如表 6 所示：

| 适配位置              | r  | WikiSQL (±0.5 %) | MNLI (±0.1 %) |
| ----------------- | -- | ---------------- | ------------- |
| 仅 $W_q$           | 1  | 68.8             | 90.7          |
| 仅 $W_q$           | 2  | 69.6             | 90.9          |
| 仅 $W_q$           | 4  | 70.5             | 91.1          |
| 仅 $W_q$           | 8  | 70.4             | 90.7          |
| 仅 $W_q$           | 64 | 70.0             | 90.7          |
| **$W_q,W_v$**     | 1  | 73.4             | 91.3          |
| $W_q,W_v$         | 2  | 73.3             | 91.4          |
| $W_q,W_v$         | 4  | 73.7             | 91.3          |
| $W_q,W_v$         | 8  | 73.8             | 91.6          |
| $W_q,W_v$         | 64 | 73.5             | 91.4          |
| $W_q,W_k,W_v,W_o$ | 1  | 74.1             | 91.2          |
| …                 | …  | …                | …             |

可见，仅用 $r=1$ 即可让 $\{W_q,W_v\}$ 在这两项任务上达到接近最优性能，进一步说明了更新 $\Delta W$ 的“**内在秩（intrinsic rank）**”极低。反之，仅适配单个矩阵 $W_q$ 时则需要稍大 $r$ 才能逼近。

---

### 6.3 更新矩阵 $\Delta W$ 与原权重 $W$ 之间的关联

论文借助**奇异值分解（SVD）**和**子空间相似度（Grassmann  距离投影）**来探究 $\Delta W$ 与 $W$ 的关联程度。

1. **$\Delta W$ 是否与 $W$ 的主要奇异向量方向对齐？**
   定义对于矩阵 $W$ 的前 $r$ 个左右奇异向量分别为 $U_W \in \mathbb{R}^{d\times r}, V_W \in \mathbb{R}^{k\times r}$；对于 $\Delta W$ 的对应前 $r$ 个奇异向量为 $U_{\Delta}, V_{\Delta}$。论文计算了

   $$
   \lVert U_{\Delta}^\top\,W\,V_{\Delta} \rVert_F
   \quad\text{与}\quad
   \lVert W \rVert_F,\;\lVert \Delta W \rVert_F
   $$

   的对比，发现 **$\Delta W$ 与 $W$ 之间存在明显相关性**，即 $\lVert U_{\Delta}^\top\,W\,V_{\Delta} \rVert_F$ 远大于对随机矩阵情形，说明 $\Delta W$ 更倾向于放大 $W$ 中尚未充分强调但对下游任务重要的特征模式。举例：在 GPT-3 第 48 层 $W_q$ 上，当 $r=4$ 时，有

   $$
   \frac{\|\Delta W_q\|_F}{\|U_{\Delta}^\top\,W_q\,V_{\Delta}\|_F} \approx 21.5,
   \quad
   \lVert W_q\rVert_F \approx 61.95,\;\lVert \Delta W_q\rVert_F \approx 6.91,\;\lVert U^\top W_q V\rVert_F \approx 0.32,
   $$

   而随机矩阵投影仅约 0.02，可见 $\Delta W$ 在特征放大方面的力度远超随机噪声。

2. **不同秩 $r$ 学得的子空间是否一致？**
   论文考察了在相同层（如 GPT-3 第 48 层）下，$\Delta W$ 当 $r=8$ 与 $r=64$ 时得到的奇异值向量子空间之间的重叠程度，计算归一化子空间相似度 $\phi(\Delta W_{r=8}, \Delta W_{r=64})$，发现**最重要的前几维方向（尤其第 1 奇异向量）高度重合**，而后续方向多为噪声。由此进一步佐证：$\Delta W$ 的“有效秩”非常低。

3. **不同随机种子学得的 $\Delta W$ 子空间是否稳定？**
   当 $r=64$ 时，论文对两次随机初始化训练得到的 $\Delta W$ 做同样的子空间相似度分析，发现**前数个奇异方向在不同种子之间具有一定重叠**，而随机高秩矩阵之间却无此现象。说明 LoRA 的优化结果并非完全随机，而是在低秩子空间里学得了较稳定的模式。

---

## 7. 结论与展望

### 7.1 主要贡献

1. 提出 **LoRA**，利用低秩分解技术，仅对部分权重层注入 $B\in \mathbb{R}^{d \times r}$、$A\in \mathbb{R}^{r \times d}$ 两个低秩矩阵进行微调，而冻结大部分预训练权重，从而显著降低可训练参数量（如 GPT-3 175B 仅需 $\sim4.7$ M），同时在推理阶段不引入额外时延。
2. 在多种模型（RoBERTa、DeBERTa、GPT-2、GPT-3）和任务（NLU、NLG）上全面实验，证明 LoRA 在仅用数百万参数的情况下，能够**获得或超过全量微调的性能**，并优于其他高效微调方法（Adapter、Prefix 等）。
3. 通过实证分析，揭示了**微调更新矩阵 $\Delta W$ 的“内在秩”极低**，并与原权重 $W$ 具有显著相关性，从而从**理论和经验**两方面说明 LoRA 原理的合理性。

### 7.2 未来研究方向

论文指出若干可拓展方向：

1. **与其他高效微调方法结合**：如将 LoRA 与 Prefix-Tuning、Adapter、Tensor-Product 结构等组合，或利用不同张量分解方法进一步压缩参数空间。
2. **微调机理研究**：深入探索为何低秩更新能够捕获下游任务所需特征，以及 $\Delta W$ 如何重塑预训练特征空间。LoRA 的低秩约束使得此类分析更易开展。
3. **自动化权重选取**：目前论文主要通过经验或少量验证选取在哪些权重矩阵上应用 LoRA；未来可研究更**有理论依据**或通过自动搜索的方式来决定最优注入位置。
4. **进一步压缩与加速**：针对不同任务的秩需求进行动态调节，或探索更稀疏/结构化的低秩分解形式，以在更严格资源约束下保持性能。

总之，LoRA 在保持推理效率的前提下，实现了对大规模预训练模型微调的**极大参数压缩**，为大模型在资源受限环境下的下游部署提供了可行方案，并为进一步研究微调机理提供了启发。

[论文中文译文](/pdf/28.LoRA-2106.09685v2.ch.pdf)

<iframe
  src="/pdf/Advanced_Battle_Management_System.pdf"
  width="100%"
  height="600"
  style="border:1px solid #ccc;"
>
  此浏览器不支持 iframe，请  
  <a href="/pdf/28.LoRA-2106.09685v2.ch.pdf">点击下载 PDF</a>
</iframe>


